{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:** Understand maximum likelihood estimation and how to use it to estimate parameters of probability distributions using observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Maximum likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we have seen how to generate data from a probability distribution. Usually the underlying probability distribution depends on some parameters, such as $\\mu$ and $\\sigma$. As before, let's bundle up those parameters into a vector $\\theta = [\\theta_0, \\theta_1, \\ldots]$, so that the probability distribution is $P(\\theta)$. If the random variable $x$ is drawn from this distribution, we would write:\n",
    "\n",
    "$$ x \\sim P(\\theta) $$\n",
    "\n",
    "If $x$ is continuous, the **probability density function** would be a function of both $x$, conditional on $\\theta$:\n",
    "\n",
    "$$P(x \\mid \\theta)$$\n",
    "\n",
    "We expect this function to integrate to unity:\n",
    "\n",
    "$$ \\int P(x \\mid \\theta) dx = 1 $$\n",
    "\n",
    "We have seen previously that **statistical inference** allows us to find an estimate for $\\theta$, which we again refer to as $\\hat{\\theta}$. To do that, we need an **estimator** for $\\hat{\\theta}$, and preferrable one that has a low mean-squared error. One very here approach for performing such inference is called [maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood). The foundation of this method is to interpret the above probability distribution in slightly different manner. Instead, we introduce the likelihood of a single data point $x$ as:\n",
    "\n",
    "$$ \\mathcal{L}(\\theta \\mid x) = P(x \\mid \\theta) $$\n",
    "\n",
    "This is the *likelihood* of $\\theta$ given our data $x$. For multiple, independent samples $[x_0, x_1, \\ldots]$ the likelihood is simply the product:\n",
    "\n",
    "$$ \\mathcal{L}(\\theta \\mid x_0, x_1, \\ldots) = \\prod_i \\mathcal{L}(\\theta \\mid x_i) = \\prod_i P(x_i \\mid \\theta) $$\n",
    "\n",
    "The maximum likelihood method consists of finding the value of $\\theta$ that maximizes this likelihood $ \\mathcal{L}(\\theta \\mid x_0, x_1, \\ldots) $. It is often more natural to maximize the natural log of this function, or the *log-likelihood*:\n",
    "\n",
    "$$ ln \\mathcal{L}(\\theta \\mid x_0, x_1, \\ldots) $$\n",
    "\n",
    "Or, we can throw in a minus sign and *minimize the negative log-likelihood*:\n",
    "\n",
    "$$ -ln \\mathcal{L}(\\theta \\mid x_0, x_1, \\ldots) = - \\sum_i ln P(x_i \\mid \\theta) $$\n",
    "\n",
    "Here is an outline of the process in words:\n",
    "\n",
    "1. Find the PDF of our model.\n",
    "2. For each observed data point $x_i$ evaluate the PDF at that point.\n",
    "3. Take its negative natural log.\n",
    "4. Sum up those values for all data points.\n",
    "5. Find the value of $\\theta$ that minimizes the sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLE for the normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how MLE works for the normal distribution. The probability density function is:\n",
    "\n",
    "$$ P(x \\mid \\mu,\\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp{\\left[-\\frac{(x-\\mu)^2}{2 \\sigma^2}\\right]}$$\n",
    "\n",
    "The log-liklihood for a single sample is then:\n",
    "\n",
    "$$ ln \\mathcal{L}(\\mu,\\sigma^2 \\mid x) =  - \\frac{1}{2} ln(2 \\pi \\sigma^2) - \\frac{(x-\\mu)^2}{2 \\sigma^2} $$\n",
    "\n",
    "For an array of values $[x_1, x_2, \\ldots]$ the negative log-likelihood is then:\n",
    "\n",
    "$$ -ln \\mathcal{L}(\\mu,\\sigma^2 \\mid x_1, x_2, \\ldots) = \\frac{n}{2} ln(2 \\pi \\sigma^2) + \\frac{1}{2 \\sigma^2}\\sum_{i=1}^n (x_i-\\mu)^2 $$\n",
    "\n",
    "This is the function we need to minimize to find our estimates $\\hat{\\mu}$ and $\\hat{\\sigma}^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 5.0\n",
    "sigma2 = 2.0\n",
    "data = np.random.normal(mu, np.sqrt(sigma2), 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our distribution of generated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc8a7b8db38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data, bins=20, normed=True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Generated Sample Distribution for the Normal Distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative log-likelihood depends on two things: the data and the vector of parameters $\\theta$. Let's write a Python function that returns the negative log-likelihood for a value of $\\theta$ and an array of observed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_llh(theta, data):\n",
    "    \"\"\"Return the negative log-likelihood for the normal distribution.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta: tuple\n",
    "        The parameters [mu, sigma**2] of the normal distribution.\n",
    "    data: ndarray\n",
    "        An array of data points that are being modelled by the normal distribution.\n",
    "    \"\"\"\n",
    "    mu = theta[0]\n",
    "    sigma2 = theta[1]\n",
    "    n = len(data)\n",
    "    result = 0.5*n*np.log(2.0*np.pi*sigma2) + (1.0/(2.0*sigma2))*np.sum((data-mu)**2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479.0480019957659"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_log_llh([1.0,1.0], data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Minimization by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, to help us get intuition about this minimzation process, let's try to minimize this by hand using `interact`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_log_llh(mu, sigma2):\n",
    "    print(neg_log_llh((mu, sigma2), data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2632591a93d46ceb590de14b395a57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(FloatSlider(value=5.0, description='mu', max=10.0), FloatSlider(value=5.0, description='sigma2', max=10.0, min=0.1), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact(print_log_llh, mu=(0.0, 10.0, 0.1), sigma2=(0.1, 10.0, 0.1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Minimization using `scipy.optimize`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scipy.optimize` package contains a number of numerical optimization algorithms. In particular, the `minimize` function provides a very general interface to a number of algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `minimize` function takes two arguments:\n",
    "\n",
    "* A Python function that depends on an array of parameters $\\theta$.\n",
    "* An initial guess for that array of parameters.\n",
    "\n",
    "The trick in using this for MLE is that our negative log-likelihood depends on both the observed data and parameters. But remember, the whole point is that we already know the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.10050083,  6.13935205,  6.11116119,  5.3025816 ,  6.57701175,\n",
       "        5.44763696,  5.27047997,  4.6872846 ,  4.54511358,  5.58383849,\n",
       "        3.64630632,  2.38136421,  5.34016793,  6.04728042,  5.1514908 ,\n",
       "        2.93341037,  3.97965277,  4.21088698,  5.41223823,  5.83673974,\n",
       "        4.85654338,  6.32098933,  6.9905069 ,  4.34404638,  2.63091757,\n",
       "        2.2825338 ,  5.54673972,  6.94600969,  3.81654039,  3.89299975,\n",
       "        4.34182862,  5.25830147,  7.78897148,  2.9319309 ,  3.64921081,\n",
       "        4.58349329,  3.57927846,  7.25189337,  5.36887912,  8.00706088,\n",
       "        6.73255604,  3.09902992,  4.38633909,  5.96048381,  4.63694125,\n",
       "        4.40225664,  6.56692794,  2.993174  ,  5.97883263,  2.92466066])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a function of just $\\theta$, you can create a lambda that is a closure over the data and pass that as the first argument to `minimize`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 90.074551093132\n",
       " hess_inv: array([[ 0.043554  , -0.00033798],\n",
       "       [-0.00033798,  0.18460015]])\n",
       "      jac: array([  0.00000000e+00,  -9.53674316e-07])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 36\n",
       "      nit: 7\n",
       "     njev: 9\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([ 4.89548751,  2.14921986])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit = minimize(lambda theta: neg_log_llh(theta, data), [4.0,2.0])\n",
    "fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated parameters are stored in the `x` attribute of the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu:        4.89548750828\n",
      "sigma**2:  2.14921986083\n"
     ]
    }
   ],
   "source": [
    "print('mu:       ', fit.x[0])\n",
    "print('sigma**2: ', fit.x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the true values we are estimating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu:        5.0\n",
      "sigma**2:  2.0\n"
     ]
    }
   ],
   "source": [
    "print('mu:       ', mu)\n",
    "print('sigma**2: ', sigma2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the case of the normal distribution, the sample mean and variance are also estimators for these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample mu:       4.89548752164\n",
      "sample sigma**2: 2.14921998216\n"
     ]
    }
   ],
   "source": [
    "print('sample mu:      ', data.mean())\n",
    "print('sample sigma**2:', data.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the MLE values are almost identical. This is because the MLE approach an be used to analytically derive these estimators, by taking derivatives of the negative log-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Minimization using `scipy.stats`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scipy.stats` modules has a set of nice objects that represent different probability distributions. These objects go beyond the capabilities in `numpy.random` and provide:\n",
    "\n",
    "* Methods to evaluate the PMF or PDF.\n",
    "* Methods to evaluate the log-likelihood.\n",
    "* Methods to fit the MLE on data.\n",
    "* Many others.\n",
    "\n",
    "For more information, see the `scipy.stats` [documentation](https://docs.scipy.org/doc/scipy-0.18.1/reference/stats.html).\n",
    "\n",
    "For now, lets look at the normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the parameters are known, you can create an object that represents the distributions with those parameters fixed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = norm(mu, np.sqrt(sigma2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can use various methods to query the distribution (with those parameters fixed). Here is the PDF $P(x)$ evaluated at $x=1.0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0051667463385230176"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N.pdf(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the natural log of the pdf (which is useful in MLE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.2655121234846449"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N.logpdf(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like `numpy.random.rand` we can also generate samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.51057758,  6.36476531,  4.33983582,  2.59425274,  5.42743955,\n",
       "        8.1665196 ,  6.67834238,  7.16961793,  5.87674665,  7.07729757])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N.rvs(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, all of these usage case assume we have fixed the parameters at the beginning by instantiating the norm object using `norm(parameters...)`. We can also perform MLE given known data using the `fit` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_hat, sigma_hat = norm.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that the MLE here returns an estimate for $\\sigma$ rather than $\\sigma^2$. But the values are the same as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample mu:       4.89548752164\n",
      "sample sigma**2: 2.14921998216\n"
     ]
    }
   ],
   "source": [
    "print('sample mu:      ', mu_hat)\n",
    "print('sample sigma**2:', sigma_hat**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Minimization using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's perform MLE using TensorFlow. While this is overkill, it shows the general pattern that we are going to use in all modelling situations:\n",
    "\n",
    "1. Pick a model $M$ with parameters $\\theta$.\n",
    "2. Pick a Performance Metric $P$ that we wish to minimize/maximize.\n",
    "3. Use an optimization algorithm to find the estimated parameter vector $\\hat{\\theta}$ that minimizes/maximizes $P$.\n",
    "4. Use the estimated parameter vector $\\hat{\\theta}$ with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import distributions as dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is to create a model that represents the normal distribution and MLE on its parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist.Normal??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = tf.Variable(1.0, dtype=tf.float64)     # Parameter\n",
    "sigma = tf.Variable(1.0, dtype=tf.float64) # Parameter\n",
    "x = tf.placeholder(dtype=tf.float64)       # Use a placeholder to represent observed data\n",
    "model = dist.Normal(mu, sigma)       # Normal distribution\n",
    "nll = tf.reduce_sum(-1.0*model.log_pdf(x))  # Performance metric = negative log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the standard initialization of the TensorFlow `Session`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to train our model by minimizing the negative log-likelihood. To do this, we need to instantiate an optimizer from `tf.train`. In this case we will use a `GradientDescentOptimizer` with a learning rate of `0.01`. The [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) optimization algorithm works well with large datasets and large numbers of parameters. In our case, it is overkill, but it is nonetheless useful to see how it works for later usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = optimizer.minimize(nll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will train our model. Conceptually, the gradient descent algorithm computes the gradient of the performance metric with respect to the parameters. It then takes a step in the most downhill direction. The optimization API for the `GradientDescentOptimizer` reflects this step-wise approach so we have to take 100 steps explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    sess.run(train, {x: data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have trained the model, we can run the computational graph for $\\hat{\\mu}$ and $\\hat{\\sigma}$ itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.8010652773425537, 1.3323128800523563]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run([mu, sigma])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the value of our performance metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.292748633678798"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(nll, {x: data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power here is that we have a single, uniform API (`Session.run`) for querying about the model. Amazingly, this same API can be used to train a wide range of models, including state of the art deep neural networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {
    "dcab71cd34ac42719a02872c0fd055fc": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
